{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5147765,"sourceType":"datasetVersion","datasetId":2990858}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch  # PyTorch library for tensor computations and deep learning\nimport torch.nn as nn  # Neural network modules and layers\nimport torch.optim as optim  # Optimization algorithms for training\nimport torchvision  # PyTorch's computer vision library\nfrom torchvision import datasets, transforms as T  # Datasets and image transformations\nfrom torch.utils.data import DataLoader, SubsetRandomSampler  # Data loading utilities\nimport numpy as np  # Numerical operations on arrays\nimport timm  # PyTorch Image Models library with pretrained models, including Vision Transformers\nimport os  # Operating system interface for file and directory operations\nimport matplotlib.pyplot as plt  # Plotting library for visualizations\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-29T02:08:28.610695Z","iopub.execute_input":"2024-09-29T02:08:28.611428Z","iopub.status.idle":"2024-09-29T02:08:28.617183Z","shell.execute_reply.started":"2024-09-29T02:08:28.611386Z","shell.execute_reply":"2024-09-29T02:08:28.616217Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_size = 224\n\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomRotation(15),\n    T.ColorJitter(brightness=0.1, contrast=0.1, saturation = 0.1),\n    T.Resize((image_size, image_size)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-09-29T02:08:30.112992Z","iopub.execute_input":"2024-09-29T02:08:30.11352Z","iopub.status.idle":"2024-09-29T02:08:30.12214Z","shell.execute_reply.started":"2024-09-29T02:08:30.113483Z","shell.execute_reply":"2024-09-29T02:08:30.12114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = '/kaggle/input/eurosat10-classes/EuroSAT_RGB/'  # Path to the dataset directory\n\n# Create a dataset from the images in the specified directory, applying the defined transformations\ndataset = datasets.ImageFolder(root=data_dir, transform=transform)\n\nclasses = dataset.classes\nprint(\"Classes:\", classes)\n\nbatch_size = 32\nvalid_size = 0.2\n\nnum_data = len(dataset)\nindices = list(range(num_data))\nnp.random.shuffle(indices)\n\nsplit = int(np.floor(valid_size * num_data))\n\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_idx) # Randomly samples elements from the training indices\nvalid_sampler = SubsetRandomSampler(valid_idx)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T02:15:31.914296Z","iopub.execute_input":"2024-09-29T02:15:31.914733Z","iopub.status.idle":"2024-09-29T02:15:36.249644Z","shell.execute_reply.started":"2024-09-29T02:15:31.91468Z","shell.execute_reply":"2024-09-29T02:15:36.248676Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\nvalid_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True)\nnum_classes = len(classes)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.head = nn.Linear(model.head.in_features, num_classes)\n\nfor param in model.blocks[-1].parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-09-29T02:24:18.995363Z","iopub.execute_input":"2024-09-29T02:24:18.995818Z","iopub.status.idle":"2024-09-29T02:24:26.489599Z","shell.execute_reply.started":"2024-09-29T02:24:18.995777Z","shell.execute_reply":"2024-09-29T02:24:26.488752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.AdamW([\n    {'params': model.head.parameters(), 'lr': 1e-3},\n    {'params' : model.blocks[-1].parameters(), 'lr' : 1e-4}\n], weight_decay=1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T02:36:37.3113Z","iopub.execute_input":"2024-09-29T02:36:37.312025Z","iopub.status.idle":"2024-09-29T02:36:37.684948Z","shell.execute_reply.started":"2024-09-29T02:36:37.311967Z","shell.execute_reply":"2024-09-29T02:36:37.684085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max= 10)\n\nnum_epochs = 5\n\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\nbest_val_acc = 0.0\nbest_model_path = 'best_vit_eurosat.pth'\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n    epoch_loss = running_loss / len(train_idx) #avg epoch loss\n    epoch_acc = 100. * correct / total\n    train_loss.append(epoch_loss)\n    train_accuracies.append(epoch_acc)\n    \n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for images, labels in valid_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            val_total += labels.size(0)\n            val_correct += predicted.eq(labels)sum().item()\n    \n    val_epoch_loss = val_loss / len(valid_idx)\n    val_epoch_acc = 100. * val_correct / val_total\n    val_losses.append(val_epoch_loss)\n    val_accuracies/append(val_epoch_acc)\n    \n    scheduler.step()\n    \n    print(\"Epoch\")\n    \n    if val_epoch_acc > best_val_acc:\n        best_val_acc = bal_epoch_acc\n        torch.save(model.stage_dict(), best_model_path)\n        print(\"Saved Best Model\")","metadata":{},"outputs":[],"execution_count":null}]}